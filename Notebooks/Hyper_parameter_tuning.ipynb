{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b6f919",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dde7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "N_ITER_PER_BLOCK = 200\n",
    "CV_SPLITS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "feature_cols = [\n",
    "    \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "    \"Previous\", \"TotalFleet\"\n",
    "]\n",
    "context_cols = [\"CITY_AREA\", \"POPULATION\", \"Population_density\"]\n",
    "cars_per_surface = [\n",
    "    \"CARS_PER_KM2\", \"EURO_1_PER_KM2\", \"EURO_2_PER_KM2\", \"EURO_3_PER_KM2\", \"EURO_4_PER_KM2\",\n",
    "    \"EURO_5_PER_KM2\", \"EURO_6_PER_KM2\", \"EURO_CLEAN_PER_KM2\", \"Previous_PER_KM2\"\n",
    "]\n",
    "cars_per_capita = [\n",
    "    \"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "    \"EURO_3_PER_CAPITA\", \"EURO_4_PER_CAPITA\", \"EURO_5_PER_CAPITA\", \"EURO_6_PER_CAPITA\",\n",
    "    \"EURO_CLEAN_PER_CAPITA\", \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "blocks = {\n",
    "    'block_1': feature_cols,\n",
    "    'block_2': feature_cols + context_cols,\n",
    "    'block_3': feature_cols + context_cols + cars_per_capita,\n",
    "    'block_4': feature_cols + cars_per_capita,\n",
    "    'block_5': feature_cols + context_cols + cars_per_surface,\n",
    "    'block_6': feature_cols + cars_per_surface,\n",
    "    'block_7': feature_cols + context_cols + cars_per_surface + cars_per_capita,\n",
    "    'block_8': feature_cols + cars_per_surface + cars_per_capita\n",
    "}\n",
    "\n",
    "outer_cv = KFold(CV_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "dataset_path = os.path.join('..', 'Data', 'Final_Dataset', 'final_dataset.parquet')\n",
    "results_list = []\n",
    "\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]\n",
    "raw = dataset.to_pandas()\n",
    "\n",
    "\n",
    "fit_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41117f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant_col in pollutants_cols:\n",
    "    pollutant = pollutant_col.split('_')[0][7:]\n",
    "    raw_aux = raw[raw[pollutant_col].notna()].reset_index(drop=True)\n",
    "    \n",
    "    for name, cols in blocks.items():\n",
    "        print(f'Tuning pollutant {pollutant} with {name} block')\n",
    "        \n",
    "        missing_cols = [col for col in cols if col not in raw_aux.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"Missing columns: {missing_cols}\")\n",
    "            available_cols = [col for col in cols if col in raw_aux.columns]\n",
    "            if not available_cols:\n",
    "                print(f\"Skipping {name}: No valid columns\")\n",
    "                continue\n",
    "            cols = available_cols\n",
    "        \n",
    "        X = raw_aux[cols]\n",
    "        y = raw_aux[pollutant_col]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=TEST_SIZE, random_state=42, shuffle=True\n",
    "        )\n",
    "\n",
    "        est = LGBMRegressor(\n",
    "            objective='regression',\n",
    "            metric='rmse',\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "            verbosity=-1,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "        )\n",
    "        \n",
    "        param_dist = {\n",
    "            'num_leaves': sp_randint(2, 66),\n",
    "            'max_depth': sp_randint(3, 51),\n",
    "            'learning_rate': sp_uniform(0.01, 0.15),\n",
    "            'n_estimators': sp_randint(100, 601),\n",
    "            'min_child_samples': sp_randint(1, 51),\n",
    "            'reg_alpha': sp_uniform(0.0, 2.0),\n",
    "            'reg_lambda': sp_uniform(0.0, 2.0),\n",
    "            'colsample_bytree': sp_uniform(0.6, 0.4),\n",
    "            'subsample': sp_uniform(0.6, 0.4),\n",
    "            'min_split_gain': sp_uniform(0.0, 0.5),\n",
    "            'boosting_type': ['gbdt']\n",
    "        }\n",
    "        \n",
    "        search = RandomizedSearchCV(\n",
    "            est, param_dist,\n",
    "            n_iter=N_ITER_PER_BLOCK,\n",
    "            cv=outer_cv,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            random_state=1,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            search.fit(X_train, y_train)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            cv_rmse = -search.best_score_\n",
    "            \n",
    "            best_model = search.best_estimator_\n",
    "            y_pred_test = best_model.predict(X_test)\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "            \n",
    "            overfitting_gap = test_rmse - cv_rmse\n",
    "            overfitting_ratio = test_rmse / cv_rmse if cv_rmse > 0 else np.inf\n",
    "            \n",
    "            fits_this_search = N_ITER_PER_BLOCK * CV_SPLITS\n",
    "            fit_counter += fits_this_search\n",
    "            \n",
    "            results_list.append({\n",
    "                'pollutant': pollutant,\n",
    "                'block': name,\n",
    "                'cv_rmse': cv_rmse,\n",
    "                'test_rmse': test_rmse,\n",
    "                'overfitting_gap': overfitting_gap,\n",
    "                'overfitting_ratio': overfitting_ratio,\n",
    "                'best_params': search.best_params_,\n",
    "                'duration_s': int(elapsed),\n",
    "                'n_features': len(cols),\n",
    "                'train_size': len(X_train),\n",
    "                'test_size': len(X_test),\n",
    "                'fits_performed': fits_this_search\n",
    "            })\n",
    "            \n",
    "            print(f\"[{pollutant}-{name}] CV rmse: {cv_rmse:.4f}, Test rmse : {test_rmse:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            fits_this_search = N_ITER_PER_BLOCK * CV_SPLITS\n",
    "            fit_counter += fits_this_search\n",
    "            \n",
    "            results_list.append({\n",
    "                'pollutant': pollutant,\n",
    "                'block': name,\n",
    "                'cv_rmse': np.nan,\n",
    "                'test_rmse': np.nan,\n",
    "                'overfitting_gap': np.nan,\n",
    "                'overfitting_ratio': np.nan,\n",
    "                'best_params': None,\n",
    "                'duration_s': int(time.time() - start),\n",
    "                'n_features': len(cols),\n",
    "                'train_size': len(X_train) if 'X_train' in locals() else 0,\n",
    "                'test_size': len(X_test) if 'X_test' in locals() else 0,\n",
    "                'fits_performed': fits_this_search,\n",
    "                'error': str(e)\n",
    "            })\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "output_path = os.path.join('..', 'Models', 'Results', '1st_stage.csv')\n",
    "df_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2bfae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "output_path = os.path.join('..','Models','Results','1st_stage.csv')\n",
    "feature_selections = pl.read_csv(output_path)\n",
    "\n",
    "scored = (\n",
    "    feature_selections.with_columns(\n",
    "        (100 * (1 - (pl.col(\"test_rmse\") - pl.col(\"test_rmse\").min().over(\"pollutant\"))\n",
    "                         / pl.col(\"test_rmse\").min().over(\"pollutant\")))\n",
    "        .alias(\"score\")\n",
    "    )\n",
    ")\n",
    "\n",
    "heat = (\n",
    "    scored.pivot(values=\"score\", index=\"pollutant\", columns=\"block\").sort(\"pollutant\")\n",
    ")\n",
    "\n",
    "matrix = heat.drop(\"pollutant\").to_numpy()\n",
    "\n",
    "vmin, vmax = 90, 100\n",
    "cmap = plt.get_cmap(\"Blues\").copy()\n",
    "cmap.set_under(\"lightgrey\")\n",
    "norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(matrix.shape[1]*1.4,\n",
    "                                matrix.shape[0]*0.6 + 2))\n",
    "im = ax.imshow(matrix, cmap=cmap, norm=norm)\n",
    "\n",
    "ax.set_xticks(np.arange(matrix.shape[1]),\n",
    "              labels=heat.columns[1:], rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(matrix.shape[0]),\n",
    "              labels=heat[\"pollutant\"])\n",
    "ax.set_xlabel(\"Feature block\")\n",
    "ax.set_ylabel(\"Pollutant\")\n",
    "ax.set_title(\"Percent improvement relative to the best feature-block combination \\n(colour scale 80â€“100)\")\n",
    "\n",
    "for i, pollutant in enumerate(heat[\"pollutant\"]):\n",
    "    for j, block in enumerate(heat.columns[1:]):\n",
    "        rmse = (\n",
    "            feature_selections\n",
    "            .filter((pl.col(\"pollutant\") == pollutant) &\n",
    "                    (pl.col(\"block\") == block))\n",
    "            .select(\"test_rmse\")\n",
    "            .item()\n",
    "        )\n",
    "        ax.text(j, i, f\"{rmse:.3g}\", ha=\"center\", va=\"center\",\n",
    "                color=\"black\", fontsize=8)\n",
    "fig.colorbar(im, ax=ax, shrink=0.8, label=\"% of best (90â€“100)\")\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join('..','Figures','Model_feature_selec.png')\n",
    "plt.savefig(output_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23154610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "output_path = os.path.join('..','Models','Results','1st_stage.csv')\n",
    "feature_selections = pl.read_csv(output_path)\n",
    "\n",
    "feature_selections.filter(pl.col('block') == 'per_capita_non_context').select(['pollutant', 'overfitting_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef0ea3",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77fa776",
   "metadata": {},
   "source": [
    "### 1st iteration (2nd stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9935ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "N_ITER = 500\n",
    "CV_SPLITS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "outer_cv = KFold(CV_SPLITS, shuffle=True, random_state=42)\n",
    "dataset_path = os.path.join('..', 'Data', 'Final_Dataset', 'final_dataset.parquet')\n",
    "output_path = os.path.join('..', 'Models', 'Results', '2nd_stage.csv')\n",
    "results_list = []\n",
    "\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]\n",
    "raw = dataset.to_pandas()\n",
    "\n",
    "cols = [\n",
    "    \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "    \"Previous\", \"TotalFleet\", \"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "    \"EURO_3_PER_CAPITA\", \"EURO_4_PER_CAPITA\", \"EURO_5_PER_CAPITA\", \"EURO_6_PER_CAPITA\",\n",
    "    \"EURO_CLEAN_PER_CAPITA\", \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves': sp_randint(5, 50),\n",
    "    'max_depth': sp_randint(3, 12),\n",
    "    'learning_rate': sp_uniform(0.01, 0.09),\n",
    "    'n_estimators': sp_randint(100, 500),\n",
    "    'min_child_samples': sp_randint(15, 100),\n",
    "    'reg_alpha': sp_uniform(0.1, 1.9),\n",
    "    'reg_lambda': sp_uniform(0.1, 1.9),\n",
    "    'colsample_bytree': sp_uniform(0.5, 0.4),\n",
    "    'subsample': sp_uniform(0.6, 0.3),\n",
    "    'min_split_gain': sp_uniform(0.1, 0.4),\n",
    "    'boosting_type': ['gbdt']\n",
    "}\n",
    "\n",
    "fit_counter = 0\n",
    "\n",
    "for pollutant_col in pollutants_cols:\n",
    "    pollutant = pollutant_col.split('_')[0][7:]\n",
    "    raw_aux = raw[raw[pollutant_col].notna()].reset_index(drop=True)\n",
    "    \n",
    "    print(f'tuning pollutant {pollutant}')\n",
    "    \n",
    "    missing_cols = [col for col in cols if col not in raw_aux.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        available_cols = [col for col in cols if col in raw_aux.columns]\n",
    "        if not available_cols:\n",
    "            print(f\"Skipping {pollutant}: No valid columns\")\n",
    "            continue\n",
    "        cols_to_use = available_cols\n",
    "    else:\n",
    "        cols_to_use = cols\n",
    "    \n",
    "    X = raw_aux[cols_to_use]\n",
    "    y = raw_aux[pollutant_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    est = LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='rmse',\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=0.2,\n",
    "        min_child_samples=20,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5\n",
    "    )\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        est, param_dist,\n",
    "        n_iter=N_ITER,\n",
    "        cv=outer_cv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        random_state=1,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        search.fit(X_train, y_train)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        cv_rmse = -search.best_score_\n",
    "    \n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        overfitting_gap = test_rmse - cv_rmse\n",
    "        overfitting_ratio = test_rmse / cv_rmse if cv_rmse > 0 else np.inf\n",
    "        train_test_gap = test_rmse - train_rmse\n",
    "        train_test_ratio = test_rmse / train_rmse if train_rmse > 0 else np.inf\n",
    "        \n",
    "        cv_flag = \"Failed\" if overfitting_ratio > 1.0 else \"Passed\"\n",
    "        train_flag = \"Failed\" if train_test_ratio > 1.5 else \"Passed\"\n",
    "        \n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': cv_rmse,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'overfitting_ratio': overfitting_ratio,\n",
    "            'train_test_gap': train_test_gap,\n",
    "            'train_test_ratio': train_test_ratio,\n",
    "            'best_params': search.best_params_,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use),\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'fits_performed': N_ITER * CV_SPLITS\n",
    "        })\n",
    "        \n",
    "        print(f\"[{pollutant}] CV: {cv_rmse:.4f}, Train: {train_rmse:.4f}, Test: {test_rmse:.4f}\")\n",
    "        print(f\"CV/Test: {overfitting_ratio:.3f} {cv_flag}, Train/Test: {train_test_ratio:.3f} {train_flag}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error tuning {pollutant}: {e}\")\n",
    "        elapsed = time.time() - start\n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': np.nan,\n",
    "            'train_rmse': np.nan,\n",
    "            'test_rmse': np.nan,\n",
    "            'overfitting_gap': np.nan,\n",
    "            'overfitting_ratio': np.nan,\n",
    "            'train_test_gap': np.nan,\n",
    "            'train_test_ratio': np.nan,\n",
    "            'best_params': None,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use) if 'cols_to_use' in locals() else len(cols),\n",
    "            'train_size': len(X_train) if 'X_train' in locals() else 0,\n",
    "            'test_size': len(X_test) if 'X_test' in locals() else 0,\n",
    "            'fits_performed': N_ITER * CV_SPLITS,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5db27a",
   "metadata": {},
   "source": [
    "#### CO and PM10 already passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5af949",
   "metadata": {},
   "source": [
    "### 2nd iteration (3rd stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pl.read_csv(os.path.join('..','Models','Results','2nd_stage.csv'))\n",
    "feature_selections.filter(pl.col('block') == 'per_capita')['best_params'].to_list()\n",
    "pollutants = a['pollutant'].unique().to_list()\n",
    "for pollutant in pollutants:\n",
    "    params = a.filter((pl.col('pollutant') == pollutant))['best_params'].item()\n",
    "    print(f'For the pollutant {pollutant} the best parameters where:')\n",
    "    print(f'{params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4efac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "N_ITER = 500\n",
    "CV_SPLITS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "outer_cv = KFold(CV_SPLITS, shuffle=True, random_state=42)\n",
    "dataset_path = os.path.join('..', 'Data', 'Final_Dataset', 'final_dataset.parquet')\n",
    "output_path = os.path.join('..', 'Models', 'Results', '3rd_stage.csv')\n",
    "output_path = os.path.join('..', 'Models', 'Results', '2nd_stage.csv')\n",
    "results_list = []\n",
    "\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]\n",
    "raw = dataset.to_pandas()\n",
    "\n",
    "cols = [\n",
    "    \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "    \"Previous\", \"TotalFleet\", \"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "    \"EURO_3_PER_CAPITA\", \"EURO_4_PER_CAPITA\", \"EURO_5_PER_CAPITA\", \"EURO_6_PER_CAPITA\",\n",
    "    \"EURO_CLEAN_PER_CAPITA\", \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "param_dist = { \n",
    "    'num_leaves': sp_randint(15, 35),\n",
    "    'max_depth': sp_randint(6, 11),\n",
    "    'learning_rate': sp_uniform(0.01, 0.08),\n",
    "    'n_estimators': sp_randint(100, 376),\n",
    "    'min_child_samples': sp_randint(20, 51),\n",
    "    'reg_alpha': sp_uniform(0.1, 1.2),\n",
    "    'reg_lambda': sp_uniform(0.2, 1.8),\n",
    "    'colsample_bytree': sp_uniform(0.5, 0.3),\n",
    "    'subsample': sp_uniform(0.6, 0.3),\n",
    "    'min_split_gain': sp_uniform(0.1, 0.2),\n",
    "}\n",
    "\n",
    "fit_counter = 0\n",
    "\n",
    "for pollutant_col in pollutants_cols:\n",
    "    pollutant = pollutant_col.split('_')[0][7:]\n",
    "    raw_aux = raw[raw[pollutant_col].notna()].reset_index(drop=True)\n",
    "    \n",
    "    print(f'tuning pollutant {pollutant}')\n",
    "    \n",
    "    missing_cols = [col for col in cols if col not in raw_aux.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        available_cols = [col for col in cols if col in raw_aux.columns]\n",
    "        if not available_cols:\n",
    "            print(f\"Skipping {pollutant}: No valid columns\")\n",
    "            continue\n",
    "        cols_to_use = available_cols\n",
    "    else:\n",
    "        cols_to_use = cols\n",
    "    \n",
    "    X = raw_aux[cols_to_use]\n",
    "    y = raw_aux[pollutant_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    est = LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='rmse',\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=0.2,\n",
    "        min_child_samples=20,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5\n",
    "    )\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        est, param_dist,\n",
    "        n_iter=N_ITER,\n",
    "        cv=outer_cv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        random_state=1,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        search.fit(X_train, y_train)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        cv_rmse = -search.best_score_\n",
    "    \n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        overfitting_gap = test_rmse - cv_rmse\n",
    "        overfitting_ratio = test_rmse / cv_rmse if cv_rmse > 0 else np.inf\n",
    "        train_test_gap = test_rmse - train_rmse\n",
    "        train_test_ratio = test_rmse / train_rmse if train_rmse > 0 else np.inf\n",
    "        \n",
    "        cv_flag = \"Failed\" if overfitting_ratio > 1.0 else \"Passed\"\n",
    "        train_flag = \"Failed\" if train_test_ratio > 1.5 else \"Passed\"\n",
    "        \n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': cv_rmse,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'overfitting_ratio': overfitting_ratio,\n",
    "            'train_test_gap': train_test_gap,\n",
    "            'train_test_ratio': train_test_ratio,\n",
    "            'best_params': search.best_params_,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use),\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'fits_performed': N_ITER * CV_SPLITS\n",
    "        })\n",
    "        \n",
    "        print(f\"[{pollutant}] CV: {cv_rmse:.4f}, Train: {train_rmse:.4f}, Test: {test_rmse:.4f}\")\n",
    "        print(f\"CV/Test: {overfitting_ratio:.3f} {cv_flag}, Train/Test: {train_test_ratio:.3f} {train_flag}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error tuning {pollutant}: {e}\")\n",
    "        elapsed = time.time() - start\n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': np.nan,\n",
    "            'train_rmse': np.nan,\n",
    "            'test_rmse': np.nan,\n",
    "            'overfitting_gap': np.nan,\n",
    "            'overfitting_ratio': np.nan,\n",
    "            'train_test_gap': np.nan,\n",
    "            'train_test_ratio': np.nan,\n",
    "            'best_params': None,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use) if 'cols_to_use' in locals() else len(cols),\n",
    "            'train_size': len(X_train) if 'X_train' in locals() else 0,\n",
    "            'test_size': len(X_test) if 'X_test' in locals() else 0,\n",
    "            'fits_performed': N_ITER * CV_SPLITS,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9092e",
   "metadata": {},
   "source": [
    "### Not enough for the rest of the models, NO2 more or less"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8302a",
   "metadata": {},
   "source": [
    "### 3rd iteratiom (4th stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pl.read_csv(os.path.join('..','Models','Results','2nd_stage.csv'))\n",
    "feature_selections.filter(pl.col('block') == 'per_capita')['best_params'].to_list()\n",
    "pollutants = a['pollutant'].unique().to_list()\n",
    "pollutants = ['CO','PM10','NO2','PM25','O3']\n",
    "for pollutant in pollutants:\n",
    "    params = a.filter((pl.col('pollutant') == pollutant))['best_params'].item()\n",
    "    print(f'For the pollutant {pollutant} the best parameters where:')\n",
    "    print(f'{params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58190b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "N_ITER = 500\n",
    "CV_SPLITS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "outer_cv = KFold(CV_SPLITS, shuffle=True, random_state=42)\n",
    "dataset_path = os.path.join('..', 'Data', 'Final_Dataset', 'final_dataset.parquet')\n",
    "output_path = os.path.join('..', 'Models', 'Results', '4th_stage.csv')\n",
    "results_list = []\n",
    "\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]\n",
    "raw = dataset.to_pandas()\n",
    "\n",
    "cols = [\n",
    "    \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "    \"Previous\", \"TotalFleet\", \"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "    \"EURO_3_PER_CAPITA\", \"EURO_4_PER_CAPITA\", \"EURO_5_PER_CAPITA\", \"EURO_6_PER_CAPITA\",\n",
    "    \"EURO_CLEAN_PER_CAPITA\", \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves': sp_randint(18, 27),\n",
    "    'max_depth': sp_randint(5, 10),\n",
    "    'learning_rate': sp_uniform(0.02, 0.065),  \n",
    "    'n_estimators': sp_randint(300, 360),\n",
    "    'min_child_samples': sp_randint(20, 41),\n",
    "    'reg_alpha': sp_uniform(0.33, 0.66),\n",
    "    'reg_lambda': sp_uniform(0.5, 1),\n",
    "    'colsample_bytree': sp_uniform(0.5, 0.20),\n",
    "    'subsample': sp_uniform(0.6, 0.2),\n",
    "    'min_split_gain': sp_uniform(0.10, 0.11),\n",
    "    'boosting_type': ['gbdt']\n",
    "}\n",
    "\n",
    "fit_counter = 0\n",
    "\n",
    "for pollutant_col in pollutants_cols:\n",
    "    pollutant = pollutant_col.split('_')[0][7:]\n",
    "    raw_aux = raw[raw[pollutant_col].notna()].reset_index(drop=True)\n",
    "    \n",
    "    print(f'tuning pollutant {pollutant}')\n",
    "    \n",
    "    missing_cols = [col for col in cols if col not in raw_aux.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        available_cols = [col for col in cols if col in raw_aux.columns]\n",
    "        if not available_cols:\n",
    "            print(f\"Skipping {pollutant}: No valid columns\")\n",
    "            continue\n",
    "        cols_to_use = available_cols\n",
    "    else:\n",
    "        cols_to_use = cols\n",
    "    \n",
    "    X = raw_aux[cols_to_use]\n",
    "    y = raw_aux[pollutant_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    est = LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='rmse',\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=0.2,\n",
    "        min_child_samples=20,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5\n",
    "    )\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        est, param_dist,\n",
    "        n_iter=N_ITER,\n",
    "        cv=outer_cv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        random_state=1,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        search.fit(X_train, y_train)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        cv_rmse = -search.best_score_\n",
    "    \n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        overfitting_gap = test_rmse - cv_rmse\n",
    "        overfitting_ratio = test_rmse / cv_rmse if cv_rmse > 0 else np.inf\n",
    "        train_test_gap = test_rmse - train_rmse\n",
    "        train_test_ratio = test_rmse / train_rmse if train_rmse > 0 else np.inf\n",
    "        \n",
    "        cv_flag = \"Failed\" if overfitting_ratio > 1.0 else \"Passed\"\n",
    "        train_flag = \"Failed\" if train_test_ratio > 1.5 else \"Passed\"\n",
    "        \n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': cv_rmse,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'overfitting_ratio': overfitting_ratio,\n",
    "            'train_test_gap': train_test_gap,\n",
    "            'train_test_ratio': train_test_ratio,\n",
    "            'best_params': search.best_params_,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use),\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'fits_performed': N_ITER * CV_SPLITS\n",
    "        })\n",
    "        \n",
    "        print(f\"[{pollutant}] CV: {cv_rmse:.4f}, Train: {train_rmse:.4f}, Test: {test_rmse:.4f}\")\n",
    "        print(f\"CV/Test: {overfitting_ratio:.3f} {cv_flag}, Train/Test: {train_test_ratio:.3f} {train_flag}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error tuning {pollutant}: {e}\")\n",
    "        elapsed = time.time() - start\n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': np.nan,\n",
    "            'train_rmse': np.nan,\n",
    "            'test_rmse': np.nan,\n",
    "            'overfitting_gap': np.nan,\n",
    "            'overfitting_ratio': np.nan,\n",
    "            'train_test_gap': np.nan,\n",
    "            'train_test_ratio': np.nan,\n",
    "            'best_params': None,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use) if 'cols_to_use' in locals() else len(cols),\n",
    "            'train_size': len(X_train) if 'X_train' in locals() else 0,\n",
    "            'test_size': len(X_test) if 'X_test' in locals() else 0,\n",
    "            'fits_performed': N_ITER * CV_SPLITS,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be08a6f",
   "metadata": {},
   "source": [
    "### NO 2 passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5ffd7",
   "metadata": {},
   "source": [
    "### 4th iteration (5th_stafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pl.read_csv(os.path.join('..','Models','Results','3rd_stage.csv'))\n",
    "feature_selections.filter(pl.col('block') == 'per_capita')['best_params'].to_list()\n",
    "pollutants = a['pollutant'].unique().to_list()\n",
    "pollutants = ['CO','PM10','NO2','PM25','O3']\n",
    "for pollutant in pollutants:\n",
    "    params = a.filter((pl.col('pollutant') == pollutant))['best_params'].item()\n",
    "    print(f'For the pollutant {pollutant} the best parameters where:')\n",
    "    print(f'{params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "N_ITER = 500\n",
    "CV_SPLITS = 5\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "outer_cv = KFold(CV_SPLITS, shuffle=True, random_state=42)\n",
    "dataset_path = os.path.join('..', 'Data', 'Final_Dataset', 'final_dataset.parquet')\n",
    "output_path = os.path.join('..', 'Models', 'Results', '5th_stage.csv')\n",
    "results_list = []\n",
    "\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]\n",
    "raw = dataset.to_pandas()\n",
    "\n",
    "cols = [\n",
    "    \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "    \"Previous\", \"TotalFleet\", \"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "    \"EURO_3_PER_CAPITA\", \"EURO_4_PER_CAPITA\", \"EURO_5_PER_CAPITA\", \"EURO_6_PER_CAPITA\",\n",
    "    \"EURO_CLEAN_PER_CAPITA\", \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "param_dist = {\n",
    "    'colsample_bytree': sp_uniform(0.65, 0.14),\n",
    "    'learning_rate': sp_uniform(0.01, 0.03),\n",
    "    'max_depth': sp_randint(5, 8),\n",
    "    'min_child_samples': sp_randint(18, 21),\n",
    "    'min_split_gain': sp_uniform(0.15, 0.04),\n",
    "    'n_estimators': sp_randint(310, 350),\n",
    "    'num_leaves': sp_randint(24, 31),\n",
    "    'reg_alpha': sp_uniform(0.1, 0.4),\n",
    "    'reg_lambda': sp_uniform(0.9, 0.8),\n",
    "    'subsample': sp_uniform(0.75, 0.20),\n",
    "}\n",
    "\n",
    "fit_counter = 0\n",
    "\n",
    "for pollutant_col in pollutants_cols:\n",
    "    pollutant = pollutant_col.split('_')[0][7:]\n",
    "    raw_aux = raw[raw[pollutant_col].notna()].reset_index(drop=True)\n",
    "    \n",
    "    print(f'tuning pollutant {pollutant}')\n",
    "    \n",
    "    missing_cols = [col for col in cols if col not in raw_aux.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        available_cols = [col for col in cols if col in raw_aux.columns]\n",
    "        if not available_cols:\n",
    "            print(f\"Skipping {pollutant}: No valid columns\")\n",
    "            continue\n",
    "        cols_to_use = available_cols\n",
    "    else:\n",
    "        cols_to_use = cols\n",
    "    \n",
    "    X = raw_aux[cols_to_use]\n",
    "    y = raw_aux[pollutant_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    est = LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='rmse',\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=0.2,\n",
    "        min_child_samples=20,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5\n",
    "    )\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        est, param_dist,\n",
    "        n_iter=N_ITER,\n",
    "        cv=outer_cv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        random_state=1,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        search.fit(X_train, y_train)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        cv_rmse = -search.best_score_\n",
    "    \n",
    "        best_model = search.best_estimator_\n",
    "        \n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "        y_pred_test = best_model.predict(X_test)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "        \n",
    "        overfitting_gap = test_rmse - cv_rmse\n",
    "        overfitting_ratio = test_rmse / cv_rmse if cv_rmse > 0 else np.inf\n",
    "        train_test_gap = test_rmse - train_rmse\n",
    "        train_test_ratio = test_rmse / train_rmse if train_rmse > 0 else np.inf\n",
    "        \n",
    "        cv_flag = \"Failed\" if overfitting_ratio > 1.0 else \"Passed\"\n",
    "        train_flag = \"Failed\" if train_test_ratio > 1.5 else \"Passed\"\n",
    "        \n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': cv_rmse,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_rmse': test_rmse,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'overfitting_ratio': overfitting_ratio,\n",
    "            'train_test_gap': train_test_gap,\n",
    "            'train_test_ratio': train_test_ratio,\n",
    "            'best_params': search.best_params_,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use),\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test),\n",
    "            'fits_performed': N_ITER * CV_SPLITS\n",
    "        })\n",
    "        \n",
    "        print(f\"[{pollutant}] CV: {cv_rmse:.4f}, Train: {train_rmse:.4f}, Test: {test_rmse:.4f}\")\n",
    "        print(f\"CV/Test: {overfitting_ratio:.3f} {cv_flag}, Train/Test: {train_test_ratio:.3f} {train_flag}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error tuning {pollutant}: {e}\")\n",
    "        elapsed = time.time() - start\n",
    "        fit_counter += N_ITER * CV_SPLITS\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'cv_rmse': np.nan,\n",
    "            'train_rmse': np.nan,\n",
    "            'test_rmse': np.nan,\n",
    "            'overfitting_gap': np.nan,\n",
    "            'overfitting_ratio': np.nan,\n",
    "            'train_test_gap': np.nan,\n",
    "            'train_test_ratio': np.nan,\n",
    "            'best_params': None,\n",
    "            'duration_s': int(elapsed),\n",
    "            'n_features': len(cols_to_use) if 'cols_to_use' in locals() else len(cols),\n",
    "            'train_size': len(X_train) if 'X_train' in locals() else 0,\n",
    "            'test_size': len(X_test) if 'X_test' in locals() else 0,\n",
    "            'fits_performed': N_ITER * CV_SPLITS,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "df_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f560c7",
   "metadata": {},
   "source": [
    "### All the models passed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8619d8f0",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1231802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl \n",
    "stage2   = os.path.join('..', 'Models', 'Results', '2nd_stage.csv')\n",
    "dataset = pl.read_csv(stage2).filter((pl.col('pollutant') == 'CO') |\n",
    "                                     (pl.col('pollutant') == 'PM10'))\n",
    "\n",
    "stage4   = os.path.join('..', 'Models', 'Results', '4th_stage.csv')\n",
    "aux_dataset = pl.read_csv(stage4).filter(pl.col('pollutant')== 'NO2')\n",
    "dataset = pl.concat([dataset,aux_dataset])\n",
    "\n",
    "stage5   = os.path.join('..', 'Models', 'Results', '5th_stage.csv')\n",
    "aux_dataset = pl.read_csv(stage5).filter((pl.col('pollutant') == 'O3') |\n",
    "                                     (pl.col('pollutant') == 'PM25'))\n",
    "\n",
    "dataset = pl.concat([dataset,aux_dataset])\n",
    "dataset.write_csv(os.path.join('..','Models','results','Optimal_hyper_params.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2702b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[31]\ttrain's mape: 0.0900495\tval's mape: 0.0914304\n",
      "RMSE=0.131 | MAPE=45.14%\n",
      "PM10\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[288]\ttrain's mape: 0.138675\tval's mape: 0.183267\n",
      "RMSE=5.252 | MAPE=20.47%\n",
      "NO2\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[348]\ttrain's mape: 0.150861\tval's mape: 0.23502\n",
      "RMSE=5.300 | MAPE=23.29%\n",
      "O3\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[343]\ttrain's mape: 0.138779\tval's mape: 0.207238\n",
      "RMSE=10.605 | MAPE=20.73%\n",
      "PM25\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[342]\ttrain's mape: 0.157871\tval's mape: 0.23763\n",
      "RMSE=2.973 | MAPE=23.44%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PARAMS_CSV = os.path.join('..','Models','results','Optimal_hyper_params.csv')\n",
    "DATA = os.path.join('..','Data','Final_Dataset','final_dataset.parquet')\n",
    "MODELS_DIR  = os.path.join('..', 'Models')\n",
    "PLOTS_DIR   = os.path.join('..', 'Models','Figures')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "dataset = pd.read_parquet(DATA)\n",
    "def parse_params(s:str)->dict:\n",
    "    return eval(s,{\"__builtins__\":None},{\"np\":np})\n",
    "\n",
    "hyper_params = pd.read_csv(\n",
    "    PARAMS_CSV,\n",
    "    converters={'best_params': parse_params}\n",
    ")\n",
    "\n",
    "pollutants     = hyper_params['pollutant'].tolist()\n",
    "pollutant_cols = [c for c in dataset.columns if c.startswith('MONTHLY')]\n",
    "\n",
    "feature_cols = [\n",
    "    \"EURO_1\",\"EURO_2\",\"EURO_3\",\"EURO_4\",\"EURO_5\",\"EURO_6\",\"EURO_CLEAN\",\n",
    "    \"Previous\",\"TotalFleet\",\"CARS_PER_CAPITA\",\"EURO_1_PER_CAPITA\",\n",
    "    \"EURO_2_PER_CAPITA\",\"EURO_3_PER_CAPITA\",\"EURO_4_PER_CAPITA\",\n",
    "    \"EURO_5_PER_CAPITA\",\"EURO_6_PER_CAPITA\",\"EURO_CLEAN_PER_CAPITA\",\n",
    "    \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "\n",
    "for pollutant in pollutants:\n",
    "    pollutant_col = next((c for c in pollutant_cols if pollutant in c), None)\n",
    "    if pollutant_col is None:\n",
    "        continue\n",
    "\n",
    "    pollutant_data = dataset[dataset[pollutant_col].notna()].copy()\n",
    "    \n",
    "    if len(pollutant_data) < 20:\n",
    "        print(f\"Skipping {pollutant} due to insufficient data ({len(pollutant_data)} rows)\")\n",
    "        continue\n",
    "\n",
    "    dev_data, test_data = train_test_split(\n",
    "        pollutant_data, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    train_data, val_data = train_test_split(\n",
    "        dev_data, \n",
    "        test_size=0.1, \n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    print(f\"{pollutant}\")\n",
    "\n",
    "    X_train, y_train = train_data[feature_cols], train_data[pollutant_col]\n",
    "    X_val,   y_val   = val_data[feature_cols],   val_data[pollutant_col]\n",
    "    X_test,  y_test  = test_data[feature_cols],  test_data[pollutant_col]\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "    lgb_val   = lgb.Dataset(X_val,   label=y_val,   free_raw_data=False)\n",
    "\n",
    "    best_params = (\n",
    "        hyper_params.loc[hyper_params['pollutant'] == pollutant, 'best_params']\n",
    "        .iloc[0]\n",
    "    )\n",
    "    best_params.update(dict(\n",
    "        verbosity=-1, \n",
    "        metric='mape'))\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        lgb_train,\n",
    "        num_boost_round=10_000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(1000),\n",
    "            lgb.log_evaluation(period=0),\n",
    "            lgb.record_evaluation(evals_result),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    rmse   = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mape   = mean_absolute_percentage_error(y_test, y_pred)*100\n",
    "\n",
    "    print(f\"RMSE={rmse:.3f} | MAPE={mape:.2f}%\")\n",
    "\n",
    "    train_loss = (evals_result['train']['mape'])\n",
    "    val_loss   = (evals_result['val']['mape'])\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_loss, label='Train')\n",
    "    plt.plot(val_loss,   label='Validation')\n",
    "    plt.axvline(model.best_iteration, ls='--', c='k', label='Early-stop')\n",
    "    plt.title(f\"{pollutant.upper()} - Learning curves\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"MAPE\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, f'{pollutant}_learning_curve.png'), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    model.save_model(os.path.join(MODELS_DIR, f'{pollutant}_model.txt'),\n",
    "                     num_iteration=model.best_iteration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
