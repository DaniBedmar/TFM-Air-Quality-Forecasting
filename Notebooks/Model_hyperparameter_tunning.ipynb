{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8091bb4d",
   "metadata": {},
   "source": [
    "# Model Hyperparameter tunning\n",
    "In this notebook the hyperparameter tunning for each model is performed and visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a793ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV,KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dataset_path = os.path.join('..','Data','Final_Dataset','final_dataset.parquet')\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4240b7",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31604fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "        \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "        \"Previous\",\"TotalFleet\"]\n",
    "\n",
    "context_cols = [\"CITY_AREA\",\"POPULATION\",\"Population_density\"]    \n",
    "    \n",
    "cars_per_surface = [\n",
    "        \"CARS_PER_KM2\", \"EURO_1_PER_KM2\", \"EURO_2_PER_KM2\",\"EURO_3_PER_KM2\",\"EURO_4_PER_KM2\",\n",
    "        \"EURO_5_PER_KM2\",\"EURO_6_PER_KM2\",\"EURO_CLEAN_PER_KM2\",\"Previous_PER_KM2\"]\n",
    "    \n",
    "cars_per_capita = [\n",
    "        \"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "        \"EURO_3_PER_CAPITA\",\"EURO_4_PER_CAPITA\",\"EURO_5_PER_CAPITA\",\"EURO_6_PER_CAPITA\",\n",
    "        \"EURO_CLEAN_PER_CAPITA\",\"Previous_PER_CAPITA\"]\n",
    "\n",
    "blocks = {'basic' : feature_cols, \n",
    "         'context' : feature_cols+context_cols, \n",
    "         'per_capita': feature_cols+context_cols+cars_per_capita,\n",
    "         'per_capita_non_context': feature_cols+cars_per_capita,\n",
    "         'per_surface': feature_cols+context_cols+cars_per_surface,\n",
    "         'per_surface_non_context': feature_cols+cars_per_surface,\n",
    "         'complete':feature_cols+context_cols+cars_per_surface+cars_per_capita,\n",
    "         'complete_non_context':feature_cols+cars_per_surface+cars_per_capita}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee03def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning the pollutant CO with the per_capita_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[CO - per_capita_non_context] RMSE = 0.08705, elapsed time = 447s\n",
      "Tunning the pollutant CO with the per_surface_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[CO - per_surface_non_context] RMSE = 0.08932, elapsed time = 389s\n",
      "Tunning the pollutant CO with the complete_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[CO - complete_non_context] RMSE = 0.08774, elapsed time = 566s\n",
      "Tunning the pollutant NO2 with the per_capita_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[NO2 - per_capita_non_context] RMSE = 4.49534, elapsed time = 412s\n",
      "Tunning the pollutant NO2 with the per_surface_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[NO2 - per_surface_non_context] RMSE = 4.58809, elapsed time = 388s\n",
      "Tunning the pollutant NO2 with the complete_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[NO2 - complete_non_context] RMSE = 4.48007, elapsed time = 559s\n",
      "Tunning the pollutant O3 with the per_capita_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[O3 - per_capita_non_context] RMSE = 8.35179, elapsed time = 293s\n",
      "Tunning the pollutant O3 with the per_surface_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[O3 - per_surface_non_context] RMSE = 8.87419, elapsed time = 293s\n",
      "Tunning the pollutant O3 with the complete_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[O3 - complete_non_context] RMSE = 8.33318, elapsed time = 399s\n",
      "Tunning the pollutant PM10 with the per_capita_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[PM10 - per_capita_non_context] RMSE = 5.26041, elapsed time = 375s\n",
      "Tunning the pollutant PM10 with the per_surface_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[PM10 - per_surface_non_context] RMSE = 5.47653, elapsed time = 360s\n",
      "Tunning the pollutant PM10 with the complete_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[PM10 - complete_non_context] RMSE = 5.31997, elapsed time = 546s\n",
      "Tunning the pollutant PM25 with the per_capita_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[PM25 - per_capita_non_context] RMSE = 2.73402, elapsed time = 295s\n",
      "Tunning the pollutant PM25 with the per_surface_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[PM25 - per_surface_non_context] RMSE = 2.78656, elapsed time = 285s\n",
      "Tunning the pollutant PM25 with the complete_non_context block\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "[PM25 - complete_non_context] RMSE = 2.76467, elapsed time = 406s\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "        \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "        \"Previous\",\"TotalFleet\"]\n",
    "\n",
    "context_cols = [\"CITY_AREA\",\"POPULATION\",\"Population_density\"]    \n",
    "    \n",
    "cars_per_surface = [\n",
    "        \"CARS_PER_KM2\", \"EURO_1_PER_KM2\", \"EURO_2_PER_KM2\",\"EURO_3_PER_KM2\",\"EURO_4_PER_KM2\",\n",
    "        \"EURO_5_PER_KM2\",\"EURO_6_PER_KM2\",\"EURO_CLEAN_PER_KM2\",\"Previous_PER_KM2\"]\n",
    "    \n",
    "cars_per_capita = [\n",
    "        \"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "        \"EURO_3_PER_CAPITA\",\"EURO_4_PER_CAPITA\",\"EURO_5_PER_CAPITA\",\"EURO_6_PER_CAPITA\",\n",
    "        \"EURO_CLEAN_PER_CAPITA\",\"Previous_PER_CAPITA\"]\n",
    "\n",
    "blocks = {'basic' : feature_cols, \n",
    "         'context' : feature_cols+context_cols, \n",
    "         'per_capita': feature_cols+context_cols+cars_per_capita,\n",
    "         'per_capita_non_context': feature_cols+cars_per_capita,\n",
    "         'per_surface': feature_cols+context_cols+cars_per_surface,\n",
    "         'per_surface_non_context': feature_cols+cars_per_surface,\n",
    "         'complete':feature_cols+context_cols+cars_per_surface+cars_per_capita,\n",
    "         'complete_non_context':feature_cols+cars_per_surface+cars_per_capita}\n",
    "\n",
    "outer_cv = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "dataset_path = os.path.join('..','Data','Final_Dataset','final_dataset.parquet')\n",
    "results_list = []\n",
    "\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]\n",
    "raw = dataset.to_pandas()\n",
    "\n",
    "for pollutant_col in pollutants_cols:\n",
    "    pollutant = pollutant_col.split('_')[0][7:]\n",
    "    raw_aux = raw[raw[pollutant_col].notna()]\n",
    "    for name, cols in blocks.items():\n",
    "        print(f'Tunning the pollutant {pollutant} with the {name} block')\n",
    "        X = raw_aux[cols]\n",
    "        y = raw_aux[pollutant_col]\n",
    "\n",
    "        est = LGBMRegressor(\n",
    "            objective='regression',\n",
    "            metric='rmse',\n",
    "            random_state=42,\n",
    "            n_jobs=1,\n",
    "            verbosity=-1\n",
    "        )\n",
    "    \n",
    "        param_dist = {\n",
    "            'num_leaves':      sp_randint(2, 66),\n",
    "            'max_depth':       sp_randint(3, 51),\n",
    "            'learning_rate':   sp_uniform(0.01, 0.14),\n",
    "            'n_estimators':    sp_randint(100, 601),\n",
    "            'min_child_samples': sp_randint(1, 51),\n",
    "            'boosting_type':   ['gbdt']\n",
    "            }\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            est, param_dist,\n",
    "            n_iter=200,\n",
    "            cv=outer_cv,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            random_state=1,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "        search.fit(X, y)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        best_rmse = -search.best_score_\n",
    "        best_params = search.best_params_\n",
    "        \n",
    "        results_list.append({\n",
    "            'pollutant': pollutant,\n",
    "            'block': name,\n",
    "            'best_rmse': best_rmse,\n",
    "            'best_params': best_params,\n",
    "            'duration_s': elapsed\n",
    "        })\n",
    "        \n",
    "        print(f\"[{pollutant} - {name}] RMSE = {best_rmse:.5f}, elapsed time = {elapsed:.0f}s\")\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "output_path = os.path.join('..','Models','Results','1st_stage.csv')\n",
    "df_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67f8d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join('..','Models','Results','1st_stage2.csv')\n",
    "feature_selections = pl.read_csv(output_path)\n",
    "output_path = os.path.join('..','Models','Results','1st_stagee.csv')\n",
    "feature_selections2 = pl.concat([feature_selections,pl.read_csv(output_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed90ddfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmcolors\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1st_stage.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m feature_selections \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mread_csv(output_path)\n\u001b[1;32m      9\u001b[0m scored \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m     feature_selections\u001b[38;5;241m.\u001b[39mwith_columns(\n\u001b[1;32m     11\u001b[0m         (\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m (pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_rmse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m pl\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_rmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mover(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpollutant\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "output_path = os.path.join('..','Models','Results','1st_stage.csv')\n",
    "feature_selections = pl.read_csv(output_path)\n",
    "\n",
    "scored = (\n",
    "    feature_selections.with_columns(\n",
    "        (100 * (1 - (pl.col(\"best_rmse\") - pl.col(\"best_rmse\").min().over(\"pollutant\"))\n",
    "                         / pl.col(\"best_rmse\").min().over(\"pollutant\")))\n",
    "        .alias(\"score\")\n",
    "    )\n",
    ")\n",
    "\n",
    "heat = (\n",
    "    scored.pivot(values=\"score\", index=\"pollutant\", columns=\"block\").sort(\"pollutant\")\n",
    ")\n",
    "\n",
    "matrix = heat.drop(\"pollutant\").to_numpy()\n",
    "\n",
    "vmin, vmax = 90, 100\n",
    "cmap = plt.get_cmap(\"Blues\").copy()\n",
    "cmap.set_under(\"lightgrey\")\n",
    "norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(matrix.shape[1]*1.4,\n",
    "                                matrix.shape[0]*0.6 + 2))\n",
    "im = ax.imshow(matrix, cmap=cmap, norm=norm)\n",
    "\n",
    "ax.set_xticks(np.arange(matrix.shape[1]),\n",
    "              labels=heat.columns[1:], rotation=45, ha=\"right\")\n",
    "ax.set_yticks(np.arange(matrix.shape[0]),\n",
    "              labels=heat[\"pollutant\"])\n",
    "ax.set_xlabel(\"Feature block\")\n",
    "ax.set_ylabel(\"Pollutant\")\n",
    "ax.set_title(\"Percent improvement over best feature-block combiation \\n(colour scale 80–100)\")\n",
    "\n",
    "for i, pollutant in enumerate(heat[\"pollutant\"]):\n",
    "    for j, block in enumerate(heat.columns[1:]):\n",
    "        rmse = (\n",
    "            feature_selections\n",
    "            .filter((pl.col(\"pollutant\") == pollutant) &\n",
    "                    (pl.col(\"block\") == block))\n",
    "            .select(\"best_rmse\")\n",
    "            .item()\n",
    "        )\n",
    "        ax.text(j, i, f\"{rmse:.3g}\", ha=\"center\", va=\"center\",\n",
    "                color=\"black\", fontsize=8)\n",
    "fig.colorbar(im, ax=ax, shrink=0.8, label=\"% of best (90–100)\")\n",
    "plt.tight_layout()\n",
    "output_path = os.path.join('..','Figures','Model_feature_selec2.png')\n",
    "plt.savefig(output_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f237ac",
   "metadata": {},
   "source": [
    "## Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff5ab8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the pollutant CO the best parameters where:\n",
      "{'boosting_type': 'gbdt', 'learning_rate': np.float64(0.09908068544826885), 'max_depth': 32, 'min_child_samples': 12, 'n_estimators': 545, 'num_leaves': 61}\n",
      "For the pollutant O3 the best parameters where:\n",
      "{'boosting_type': 'gbdt', 'learning_rate': np.float64(0.1148499725344171), 'max_depth': 42, 'min_child_samples': 4, 'n_estimators': 541, 'num_leaves': 45}\n",
      "For the pollutant NO2 the best parameters where:\n",
      "{'boosting_type': 'gbdt', 'learning_rate': np.float64(0.09912461600026562), 'max_depth': 42, 'min_child_samples': 7, 'n_estimators': 576, 'num_leaves': 54}\n",
      "For the pollutant PM10 the best parameters where:\n",
      "{'boosting_type': 'gbdt', 'learning_rate': np.float64(0.03607642959287393), 'max_depth': 48, 'min_child_samples': 7, 'n_estimators': 381, 'num_leaves': 52}\n",
      "For the pollutant PM25 the best parameters where:\n",
      "{'boosting_type': 'gbdt', 'learning_rate': np.float64(0.09952452332319162), 'max_depth': 19, 'min_child_samples': 20, 'n_estimators': 365, 'num_leaves': 50}\n"
     ]
    }
   ],
   "source": [
    "feature_selections.filter(pl.col('block') == 'per_capita')['best_params'].to_list()\n",
    "pollutants = feature_selections['pollutant'].unique().to_list()\n",
    "for pollutant in pollutants:\n",
    "    params = feature_selections.filter((pl.col('pollutant') == pollutant) &\n",
    "                                 (pl.col('block') == 'per_capita'))['best_params'].item()\n",
    "    print(f'For the pollutant {pollutant} the best parameters where:')\n",
    "    print(f'{params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d67d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunning the pollutant CO with the complete_non_context block\n",
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "[CO] RMSE = 0.08699, elapsed time = 2413s\n",
      "Tunning the pollutant NO2 with the complete_non_context block\n",
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "[NO2] RMSE = 4.47082, elapsed time = 2385s\n",
      "Tunning the pollutant O3 with the complete_non_context block\n",
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "[O3] RMSE = 8.29524, elapsed time = 1826s\n",
      "Tunning the pollutant PM10 with the complete_non_context block\n",
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "[PM10] RMSE = 5.27250, elapsed time = 2233s\n",
      "Tunning the pollutant PM25 with the complete_non_context block\n",
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "[PM25] RMSE = 2.74320, elapsed time = 1840s\n"
     ]
    }
   ],
   "source": [
    "outer_cv = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "dataset_path = os.path.join('..','Data','Final_Dataset','final_dataset.parquet')\n",
    "results_list = []\n",
    "\n",
    "dataset = pl.read_parquet(dataset_path)\n",
    "pollutants_cols = [col for col in dataset.columns if col.startswith('MONTHLY')]\n",
    "raw = dataset.to_pandas()\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves':      sp_randint(42, 76),\n",
    "    'max_depth':       sp_randint(19, 56),\n",
    "    'learning_rate':   sp_uniform(0.03, 0.15),\n",
    "    'n_estimators':    sp_randint(350, 601),\n",
    "    'min_child_samples': sp_randint(1, 26),\n",
    "    'boosting_type':   ['gbdt']\n",
    "    }\n",
    "\n",
    "cols = [\n",
    "        \"EURO_1\", \"EURO_2\", \"EURO_3\", \"EURO_4\", \"EURO_5\", \"EURO_6\", \"EURO_CLEAN\",\n",
    "        \"Previous\",\"TotalFleet\",\"CARS_PER_CAPITA\", \"EURO_1_PER_CAPITA\", \"EURO_2_PER_CAPITA\",\n",
    "        \"EURO_3_PER_CAPITA\",\"EURO_4_PER_CAPITA\",\"EURO_5_PER_CAPITA\",\"EURO_6_PER_CAPITA\",\n",
    "        \"EURO_CLEAN_PER_CAPITA\",\"Previous_PER_CAPITA\"]\n",
    "\n",
    "for pollutant_col in pollutants_cols:\n",
    "    pollutant = pollutant_col.split('_')[0][7:]\n",
    "    raw_aux = raw[raw[pollutant_col].notna()]\n",
    "    print(f'Tunning the pollutant {pollutant} with the {name} block')\n",
    "    X = raw_aux[cols]\n",
    "    y = raw_aux[pollutant_col]\n",
    "\n",
    "    est = LGBMRegressor(\n",
    "        objective='regression',\n",
    "        metric='rmse',\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        verbosity=-1\n",
    "    )\n",
    "\n",
    "    param_dist = param_dist\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        est, param_dist,\n",
    "        n_iter=500,\n",
    "        cv=outer_cv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        random_state=1,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "        )\n",
    "\n",
    "    start = time.time()\n",
    "    search.fit(X, y)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    best_rmse = -search.best_score_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    results_list.append({\n",
    "        'pollutant': pollutant,\n",
    "        'best_rmse': best_rmse,\n",
    "        'best_params': best_params,\n",
    "        'duration_s': elapsed\n",
    "    })\n",
    "\n",
    "    print(f\"[{pollutant}] RMSE = {best_rmse:.5f}, elapsed time = {elapsed:.0f}s\")\n",
    "\n",
    "df_results = pd.DataFrame(results_list)\n",
    "output_path = os.path.join('..','Models','Results','2nd_stage.csv')\n",
    "df_results.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605f3d3",
   "metadata": {},
   "source": [
    "## Training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd7d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM RMSE for CO: 0.0846 and a MAPE of 19.67%\n",
      "LightGBM RMSE for NO2: 4.2901 and a MAPE of 17.47%\n",
      "LightGBM RMSE for O3: 8.6618 and a MAPE of 17.22%\n",
      "LightGBM RMSE for PM10: 4.9781 and a MAPE of 20.52%\n",
      "LightGBM RMSE for PM25: 2.5077 and a MAPE of 20.68%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error\n",
    "\n",
    "dataset = pd.read_parquet(os.path.join('..','Data','Final_Dataset','final_dataset.parquet'))\n",
    "\n",
    "def parse_params(s:str)->dict:\n",
    "    return eval(s,{\"__builtins__\":None},{\"np\":np})\n",
    "\n",
    "hyper_params = pd.read_csv(\n",
    "    os.path.join('..','Models','Results','2nd_stage.csv'),\n",
    "    converters={\"best_params\": parse_params}\n",
    ")\n",
    "\n",
    "pollutants      = hyper_params['pollutant'].tolist()\n",
    "pollutant_cols  = [c for c in dataset.columns if c.startswith('MONTHLY')]\n",
    "feature_cols    = [\n",
    "    \"EURO_1\",\"EURO_2\",\"EURO_3\",\"EURO_4\",\"EURO_5\",\"EURO_6\",\"EURO_CLEAN\",\n",
    "    \"Previous\",\"TotalFleet\",\"CARS_PER_CAPITA\",\"EURO_1_PER_CAPITA\",\n",
    "    \"EURO_2_PER_CAPITA\",\"EURO_3_PER_CAPITA\",\"EURO_4_PER_CAPITA\",\n",
    "    \"EURO_5_PER_CAPITA\",\"EURO_6_PER_CAPITA\",\"EURO_CLEAN_PER_CAPITA\",\n",
    "    \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "for pollutant in pollutants:\n",
    "    pollutant_col = next((c for c in pollutant_cols if pollutant in c), None)\n",
    "    if pollutant_col is None:\n",
    "        continue\n",
    "\n",
    "    aux = dataset[dataset[pollutant_col].notna()]\n",
    "    X, y = aux[feature_cols], aux[pollutant_col]\n",
    "\n",
    "    best_params = hyper_params.loc[\n",
    "        hyper_params['pollutant']==pollutant,'best_params'\n",
    "    ].iloc[0]\n",
    "    best_params['verbosity'] = -1\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "    lgb_val   = lgb.Dataset(X_test,  label=y_test, reference=lgb_train)\n",
    "\n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_val],\n",
    "        num_boost_round=10_000,\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join('..', 'Models',f'{pollutant}_model.txt')\n",
    "    model.save_model(model_path)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse   = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rel_rmse = rmse / y.mean() * 100\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    print(f\"LightGBM RMSE for {pollutant}: {rmse:.4f} and a MAPE of {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "165b667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's l2: 0.0289474\tval's l2: 0.0338697\n",
      "CO    | iter=   1 | RMSE=0.215 | relRMSE=66.37% | MAPE=79.89%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttrain's l2: 48.3064\tval's l2: 70.0482\n",
      "NO2   | iter=  11 | RMSE=7.137 | relRMSE=39.62% | MAPE=51.22%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttrain's l2: 99.1312\tval's l2: 199.692\n",
      "O3    | iter=  10 | RMSE=15.705 | relRMSE=31.57% | MAPE=28.09%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's l2: 52.9622\tval's l2: 48.4243\n",
      "PM10  | iter=   1 | RMSE=9.292 | relRMSE=42.61% | MAPE=35.05%\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\ttrain's l2: 11.6318\tval's l2: 15.4444\n",
      "PM25  | iter=   5 | RMSE=3.734 | relRMSE=35.71% | MAPE=31.24%\n",
      "\n",
      "✓ All pollutant models trained, evaluated, and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# ── 0. paths ────────────────────────────────────────────────────────────────\n",
    "DATA_PATH   = os.path.join('..', 'Data', 'Final_Dataset', 'final_dataset.parquet')\n",
    "PARAMS_CSV  = os.path.join('..', 'Models', 'Results', '2nd_stage.csv')\n",
    "MODELS_DIR  = os.path.join('..', 'Models')\n",
    "PLOTS_DIR   = os.path.join('..', 'Figures')\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# ── 1. load data & hyper-parameters ─────────────────────────────────────────\n",
    "dataset = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "def parse_params(s:str)->dict:\n",
    "    return eval(s,{\"__builtins__\":None},{\"np\":np})\n",
    "\n",
    "hyper_params = pd.read_csv(\n",
    "    PARAMS_CSV,\n",
    "    converters={'best_params': parse_params}\n",
    ")\n",
    "\n",
    "pollutants     = hyper_params['pollutant'].tolist()\n",
    "pollutant_cols = [c for c in dataset.columns if c.startswith('MONTHLY')]\n",
    "\n",
    "feature_cols = [\n",
    "    \"EURO_1\",\"EURO_2\",\"EURO_3\",\"EURO_4\",\"EURO_5\",\"EURO_6\",\"EURO_CLEAN\",\n",
    "    \"Previous\",\"TotalFleet\",\"CARS_PER_CAPITA\",\"EURO_1_PER_CAPITA\",\n",
    "    \"EURO_2_PER_CAPITA\",\"EURO_3_PER_CAPITA\",\"EURO_4_PER_CAPITA\",\n",
    "    \"EURO_5_PER_CAPITA\",\"EURO_6_PER_CAPITA\",\"EURO_CLEAN_PER_CAPITA\",\n",
    "    \"Previous_PER_CAPITA\"\n",
    "]\n",
    "\n",
    "# ensure datetime\n",
    "dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "\n",
    "# ── 2. time-based splits ───────────────────────────────────────────────────\n",
    "mask_dev  = dataset['date'] <  '2022-01-01'   # up to end-2021 for dev\n",
    "mask_test = dataset['date'] >= '2022-01-01'   # 2022+ for hold-out\n",
    "\n",
    "dev_df  = dataset.loc[mask_dev].copy()\n",
    "test_df = dataset.loc[mask_test].copy()\n",
    "\n",
    "split_idx = int(len(dev_df) * 0.9)\n",
    "train_df  = dev_df.iloc[:split_idx]\n",
    "val_df    = dev_df.iloc[split_idx:]\n",
    "\n",
    "# ── 3. training loop ───────────────────────────────────────────────────────\n",
    "for pollutant in pollutants:\n",
    "    pollutant_col = next((c for c in pollutant_cols if pollutant in c), None)\n",
    "    if pollutant_col is None:\n",
    "        continue\n",
    "\n",
    "    # filter out NA target rows\n",
    "    train_rows = train_df[train_df[pollutant_col].notna()]\n",
    "    val_rows   = val_df  [val_df  [pollutant_col].notna()]\n",
    "    test_rows  = test_df [test_df [pollutant_col].notna()]\n",
    "\n",
    "    X_train, y_train = train_rows[feature_cols], train_rows[pollutant_col]\n",
    "    X_val,   y_val   = val_rows  [feature_cols], val_rows  [pollutant_col]\n",
    "    X_test,  y_test  = test_rows [feature_cols], test_rows [pollutant_col]\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
    "    lgb_val   = lgb.Dataset(X_val,   label=y_val,   free_raw_data=False)\n",
    "\n",
    "    best_params = (\n",
    "        hyper_params.loc[hyper_params['pollutant'] == pollutant, 'best_params']\n",
    "        .iloc[0]\n",
    "    )\n",
    "    best_params.update(dict(verbosity=-1, metric='l2'))\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    model = lgb.train(\n",
    "        best_params,\n",
    "        lgb_train,\n",
    "        num_boost_round=10_000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(200),\n",
    "            lgb.log_evaluation(period=0),\n",
    "            lgb.record_evaluation(evals_result)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ── 4. final test-set evaluation ────────────────────────────────────────\n",
    "    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    rmse   = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rel_rmse = rmse / y_test.mean() * 100\n",
    "    mape   = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "\n",
    "    print(f\"{pollutant:5s} | iter={model.best_iteration:4d} | \"\n",
    "          f\"RMSE={rmse:.3f} | relRMSE={rel_rmse:.2f}% | MAPE={mape:.2f}%\")\n",
    "\n",
    "    # ── 5. plot learning curves ────────────────────────────────────────────\n",
    "    train_loss = np.sqrt(evals_result['train']['l2'])\n",
    "    val_loss   = np.sqrt(evals_result['val'  ]['l2'])\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_loss, label='Train')\n",
    "    plt.plot(val_loss,   label='Validation')\n",
    "    plt.axvline(model.best_iteration, ls='--', c='k', label='Early-stop')\n",
    "    plt.title(f\"{pollutant.upper()} – Learning curves\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(PLOTS_DIR, f'{pollutant}_learning_curve.png'), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    # ── 6. save model ──────────────────────────────────────────────────────\n",
    "    model.save_model(os.path.join(MODELS_DIR, f'{pollutant}_model.txt'),\n",
    "                     num_iteration=model.best_iteration)\n",
    "\n",
    "print(\"\\n✓ All pollutant models trained, evaluated, and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
